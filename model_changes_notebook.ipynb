{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from sys import platform\n",
    "\n",
    "# To make sure the folder standards work in windows and other systems\n",
    "if platform == \"win32\":\n",
    "    # Windows...\n",
    "    folder_separator = \"\\\\\"\n",
    "else:\n",
    "    folder_separator = \"/\"\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent our words as vectors, we use one-hot encoding. Each word is then a large vector full of zeroes, with a one in the position of the word. The size of the vector is the length of the vocabulary. The following helper class creates a Lang object, which is useful for making our one-hot encoded word vectors. The Lang object contains two dictionaries: a word2index dictionary that has each word of our vocabulary as keys, and its index in the vector as value. The other dictionary is index2word, which does the same but is reversed. It also keeps track of how many words we have, in a word2count dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make two Lang classes from our dataset. The Lang class stores useful information about the words and indexes in our one-hot encoded vector. We use these classes for training. The input language is going to be English, and the output language will be Dutch. Also, in this step each input sentence is reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def lang_class_maker(data, lang1, lang2):\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    \n",
    "    for pair in data:\n",
    "        # The sentence is splitted, reversed, and reassembled\n",
    "        input_sentence = \" \".join(pair[0].split()[::-1])\n",
    "        input_lang.addSentence(input_sentence)\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    return input_lang, output_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below follow the encoder, decoder and attention decoder classes, as described by the [Pytorch tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20 # This depends on the length of sentences in our sample set. For actual training it needs to be higher.\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following helper functions for our training. When given a pair of sentences, we want to make two tensors from them, containing the indexes of each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    total = list()\n",
    "    for word in sentence.split(\" \"):\n",
    "        try:\n",
    "            total.append(lang.word2index[word])\n",
    "        except:\n",
    "            print(f\"This is the word: {word}\")\n",
    "            raise Exception(\"We fucked up, and its a key error\")\n",
    "    return total\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are very useful for the human operator at the computer. The first one shows a plot with the cost per amount of iterations, which helps with determining how fast the model improves, and gives an indication of how many iterations we should use. The second one calculates the amount of time that went by, and the amount of time left until the training is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are finally ready for training, which we will do with the following two functions. The first one trains for one sentence pair. The second one iterates a certain amount of times, and trains one pair of sentences in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    # Use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can put the different tweaks to the models. The first one is to use teacher forcing. This basically uses the target sentence (given from the data) as input in the Recurrent Neural Network, so the model already knows the target sentence and tries to approximate this. The teacher forcing ratio can be specified in the input, and is a number between 0 and 1, indicating in how much of the cases we will apply teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_teacher(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, \n",
    "                  max_length=MAX_LENGTH, teacher_forcing_ratio=0.5):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, pairs, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(pairs[i]) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train_teacher(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been trained, we also want to be able to test what it's output is. The following function takes the trained encoder and decoder, and an English sentence as input, and produces a Dutch translation as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def bleu_text(target_text: list, reference_text: list, n_precision = 4, smoothing = 0):\n",
    "    \"\"\"takes a target_text, and a reference_text, of the same length, as lists within lists, where each inner list is a\n",
    "    tokenized sentence, n_precision, which determines up to which n-gram the bleu value is computed, and a possible\n",
    "    smoothing\"\"\"\n",
    "    temp = np.zeros((n_precision, 2)) #each row is an n(-gram), column 0 is counted matches, column 1 is total n-grams\n",
    "    for i in range(0, len(target_text)):\n",
    "        for j in range(0, n_precision):\n",
    "            count = 0\n",
    "            target = list(ngrams(target_text[i], j+1))\n",
    "            reference = list(ngrams(reference_text[i], j+1))\n",
    "            for x in target:\n",
    "                #to make sure no n-gram is used as a match twice, it is removed from the reference list\n",
    "                if x in reference:\n",
    "                    reference.remove(x)\n",
    "                    count += 1\n",
    "            #if a smoothing is specified, and for a certain n-gram 0 counts were found, it is smoothed over\n",
    "            if smoothing != 0 and count == 0:\n",
    "                count = smoothing\n",
    "            temp[j] += [count, len(target)]\n",
    "    #the amount of matches will be divided by the total amount of n-grams\n",
    "    result = temp[:,0]/temp[:,1]\n",
    "    #the final result will be the geometric mean of the different n-gram results\n",
    "    return result.prod()**(1/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n",
      "En/De coding...\n",
      "Training...\n",
      "Saving...\n",
      "Loading...\n",
      "En/De coding...\n",
      "(['ok√©,', 'steek', 'nu', 'over'], ['costa', 'priester', 'werken,', 'het,', 'gast', 'rapport', 'rapport', 'rapport', 'rapport', 'rapport', 'ga', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'doen', 'ga', 'doen', 'ga', 'hoe', 'doen', 'ga', 'ga', 'doen', 'ga', 'doen', 'ga', 'ga', 'mee', 'ga', 'doen', 'ga', 'ga', 'mee', 'werken,', 'werken,', 'gast', 'gast', 'rapport', 'rapport', 'rapport', 'rapport', 'telefoon', 'hoe', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'werken,', 'gast'])\n",
      "(['klopt,', 'maar', 'ik', 'probeer', 'het', 'wel'], ['gisteren', 'gisteren', 'dak', 'doen', 'ga', 'hoe', 'werken,', 'werken,', 'gast', 'rapport', 'rapport', 'rapport', 'rapport', 'rapport', 'ga', 'ga', 'doen', 'ga', 'doen', 'ga', 'ga', 'hoe', 'hoe', 'doen', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'werken,', 'werken,', 'gast', 'doen', 'gast', 'rapport', 'rapport', 'rapport', 'rapport', 'telefoon', 'hoe', 'rapport', 'ga', 'hoe', 'doen', 'ga', 'doen', 'ga', 'hoe', 'doen', 'ga', 'ga', 'doen', 'ga', 'hoe', 'doen', 'ga', 'hoe'])\n",
      "(['je', 'komt', 'binnen,', 'met', 'de', 'kinderen,', 'en', 'vertelt', 'dit', 'alles'], ['priester', 'het,', 'het,', 'weet', 'gast', 'rapport', 'rapport', 'rapport', 'rapport', 'ga', 'hoe', 'doen', 'ga', 'ga', 'doen', 'ga', 'ga', 'doen', 'ga', 'ga', 'mee', 'ga', 'mee', 'werken,', 'gast', 'doen', 'gast', 'gast', 'rapport', 'rapport', 'rapport', 'rapport', 'ga', 'ga', 'hoe', 'doen', 'ga', 'doen', 'ga', 'hoe', 'doen', 'ga', 'ga', 'doen', 'ga', 'doen', 'ga', 'hoe', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'doen', 'ga', 'hoe', 'doen', 'ga', 'ga', 'doen'])\n",
      "0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EncoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([317, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-616339a23778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_be_evaluated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_be_evaluated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mencoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models{folder_separator}model1encoder.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mattndecoder1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models{folder_separator}model1attn_decoder.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tm_project/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EncoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([605, 256]) from checkpoint, the shape in current model is torch.Size([317, 256])."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This initializes the encoder and the attention decoder. We can choose to specify the amount of hidden layers\n",
    "hidden_size = 256\n",
    "training_chunk_num = 8\n",
    "test_chunk_num = 3\n",
    "initialized = False\n",
    "show_progress = True\n",
    "pickle_off = f\"data{folder_separator}data/sample_not_a_sample.pkl\"\n",
    "\n",
    "file_stream = (open(pickle_off, \"rb\"))\n",
    "while training_chunk_num:\n",
    "    try:\n",
    "        # Read the pickle manually, to keep track of the stream\n",
    "        print(\"Loading...\")\n",
    "        pairs = pickle.load(file_stream)\n",
    "        random.shuffle(pairs)\n",
    "        input_lang, output_lang = lang_class_maker(pairs, 'en', 'nl')\n",
    "\n",
    "        # Prepare the encoder and the attention decoder\n",
    "        print(\"En/De coding...\")\n",
    "        encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "        attndecoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "        \n",
    "        # Load the previous states if its already initialized\n",
    "        if initialized:            \n",
    "            # To show progress, we can evaluate each step with the next chunk before using\n",
    "            # it on training\n",
    "            if show_progress:\n",
    "                to_be_evaluated=[[],[]]\n",
    "\n",
    "                for pair in pairs:\n",
    "                    to_be_evaluated[0].append(pair[1].split(' '))\n",
    "                    to_be_evaluated[1].append(evaluate(encoder1, attndecoder1, pair[0]))\n",
    "\n",
    "                for i in range(3):\n",
    "                    print((to_be_evaluated[0][i], to_be_evaluated[1][i]))\n",
    "\n",
    "                print(bleu_text(to_be_evaluated[0], to_be_evaluated[1]))\n",
    "            \n",
    "            encoder1.load_state_dict(torch.load(f'models{folder_separator}model1encoder.pt'))\n",
    "            attndecoder1.load_state_dict(torch.load(f'models{folder_separator}model1attn_decoder.pt'))\n",
    "        else:\n",
    "            initialized = True\n",
    "\n",
    "        # This actually trains the model. The good part about this is that you can interrupt or stop it for a bit, and run it again\n",
    "        # later, to continue the same training.\n",
    "        # Fix the third value to 10000, and the last to 200. Added a new input of pairs to be sure\n",
    "        print(\"Training...\")\n",
    "        trainIters(encoder1, attndecoder1, pairs, len(pairs), print_every=200)\n",
    "        \n",
    "        # To save the model\n",
    "        print(\"Saving...\")\n",
    "        torch.save(encoder1.state_dict(), f'models{folder_separator}model1encoder.pt')\n",
    "        torch.save(attndecoder1.state_dict(), f'models{folder_separator}model1attn_decoder.pt')\n",
    "        \n",
    "        training_chunk_num -= 1\n",
    "    # This exception shows when the set is not divided to the exact number of expected chunks\n",
    "    except EOFError:\n",
    "        raise Exception(\"You have reached the end of chunks\")\n",
    "\n",
    "\n",
    "while test_chunk_num:\n",
    "    try:\n",
    "        print(\"Testing chunk...\")\n",
    "        pairs = pickle.load(file_stream)\n",
    "        to_be_evaluated=[[],[]]\n",
    "\n",
    "        for pair in pairs:\n",
    "            to_be_evaluated[0].append(pair[1].split(' '))\n",
    "            to_be_evaluated[1].append(evaluate(encoder1, decoder1, pair[0]))\n",
    "            \n",
    "        for i in range(3):\n",
    "            print((to_be_evaluated[0][i], to_be_evaluated[1][i]))\n",
    "        \n",
    "        print(bleu_text(to_be_evaluated[0], to_be_evaluated[1]))\n",
    "        \n",
    "        test_chunk_num -= 1\n",
    "    except EOFError:\n",
    "        raise Exception(\"You have reached the end of chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
